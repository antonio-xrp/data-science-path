{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumen/Revisión\n",
    "\n",
    "### Métodos basados en conjuntos y ensacado\n",
    "Se ha comprobado que los conjuntos de árboles generalizan bien al puntuar datos nuevos. Algunos conjuntos de árboles útiles y populares son el bagging, el boosting y los bosques aleatorios. Bagging, que combina árboles de decisión utilizando muestras agregadas bootstrap. Una ventaja específica del bagging es que este método puede ser multihilo o calcularse en paralelo. La mayoría de estos ensembles se evalúan utilizando el error fuera de bolsa.\n",
    "\n",
    "### Bosque aleatorio\n",
    "El bosque aleatorio es un conjunto de árboles que tiene un enfoque similar al bagging. Su principal característica es que añaden aleatoriedad utilizando sólo un subconjunto de características para entrenar cada división de los árboles que entrena. Extra Random Trees es una implementación que añade aleatoriedad creando divisiones al azar, en lugar de utilizar una búsqueda codiciosa para encontrar variables y puntos de división.\n",
    "\n",
    "### Refuerzo\n",
    "Los métodos de refuerzo son aditivos en el sentido de que vuelven a entrenar secuencialmente los árboles de decisión utilizando las observaciones con los residuos más altos en el árbol anterior. Para ello, a las observaciones con un residuo elevado se les asigna un peso mayor.\n",
    "\n",
    "### Refuerzo de gradiente\n",
    "Las principales funciones de pérdida de los algoritmos de refuerzo son:\n",
    "\n",
    "- función de pérdida0-1, que ignora las observaciones que se han clasificado correctamente. La forma de esta función de pérdida dificulta su optimización.\n",
    "\n",
    "- Función de pérdida de refuerzoadaptativo, que tiene una naturaleza exponencial. La forma de esta función es más sensible a los valores atípicos.\n",
    "\n",
    "- Función de pérdida derefuerzo por gradiente. La implementación más común del refuerzo de gradiente utiliza una función de pérdida de probabilidad logarítmica binomial llamada desviación. Tiende a ser más robusta a los valores atípicos que AdaBoost.\n",
    "\n",
    "La naturaleza aditiva del refuerzo de gradiente lo hace propenso al sobreajuste. Esto puede solucionarse mediante validación cruzada o ajustando el número de iteraciones de refuerzo. Otros hiperparámetros a afinar son\n",
    "\n",
    "- tasa de aprendizaje (contracción)\n",
    "\n",
    "- submuestra\n",
    "\n",
    "- número de características.\n",
    "\n",
    "### Apilamiento\n",
    "El apilamiento es un método de ensamblaje que combina cualquier tipo de modelo combinando las probabilidades predichas de las clases. En ese sentido, es un caso generalizado de bagging. Las dos formas más comunes de combinar las probabilidades predichas en el apilamiento son: utilizando un voto mayoritario o utilizando pesos para cada probabilidad predicha.  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "En este contenido se exploran los métodos basados en conjuntos y las técnicas de ensamblaje de árboles, como el bagging, boosting y random forests. Estas técnicas han demostrado generalizar bien al clasificar nuevos datos. A continuación, se presentan los puntos clave de cada método:\n",
    "\n",
    "1. Bagging:\n",
    "\n",
    "    - Combina árboles de decisión utilizando muestras| agregadas mediante bootstrap.\n",
    "    - Una ventaja específica del bagging es que puede ser multihilo o computarse en paralelo.\n",
    "    - Se evalúa utilizando el error out-of-bag.\n",
    "\n",
    "2. Random Forest:\n",
    "\n",
    "    - Es un ensamblaje de árboles similar al bagging, pero con la adición de aleatoriedad.\n",
    "    - Utiliza un subconjunto de características para entrenar cada división de los árboles.\n",
    "    - La implementación Extra Random Trees agrega aleatoriedad creando divisiones al azar en lugar de utilizar una búsqueda ávida.\n",
    "\n",
    "3.Boosting:\n",
    "\n",
    "    - Los métodos de boosting son aditivos, es decir, reentrenan secuencialmente árboles de decisión utilizando las observaciones con los residuos más altos del árbol anterior.\n",
    "    - Las observaciones con residuos altos se les asigna un peso mayor.\n",
    "    - Las funciones de pérdida más comunes para los algoritmos de boosting son la función de pérdida 0-1, la función de pérdida de boosting adaptativo y la función de pérdida de boosting por gradiente.\n",
    "\n",
    "4. Gradient Boosting:\n",
    "\n",
    "    - Utiliza una función de pérdida de log-verosimilitud binomial llamada deviance.\n",
    "    - Es más robusto a los valores atípicos que AdaBoost.\n",
    "    - La naturaleza aditiva del gradient boosting lo hace propenso al sobreajuste, por lo que se recomienda utilizar validación cruzada o ajustar el número de iteraciones de boosting.\n",
    "\n",
    "5. Stacking:\n",
    "    - Es un método de ensamblaje que combina cualquier tipo de modelo al combinar las probabilidades predichas de las clases.\n",
    "    - Se puede combinar utilizando una votación mayoritaria o asignando pesos a cada probabilidad predicha.\n",
    "    \n",
    "Recuerda que el aprendizaje es un proceso y es normal tomar tiempo para comprender completamente estos conceptos. ¡Estoy aquí para apoyar tu viaje de aprendizaje! Si tienes alguna pregunta adicional, no dudes en hacerla.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
