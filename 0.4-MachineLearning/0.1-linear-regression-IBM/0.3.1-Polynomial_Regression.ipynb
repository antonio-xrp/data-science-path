{"cells":[{"cell_type":"markdown","id":"c25b988c-dcfb-4763-ae34-881f6ec2d41d","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"a37f6ed9-a977-4b0c-8a7e-c897dd4cce2a","metadata":{},"outputs":[],"source":["# Polynomial Regression\n","\n","\n","Estimated time needed: **40** minutes\n","\n","What if your data is more complex than a straight line? Surprisingly, you can use a linear model to fit nonlinear data. A simple way to do this is to add powers of each feature as new features, then train a linear model on this extended set of features. This technique is called Polynomial Regression.\n","\n","There are two factors when determining model performance: overfitting and underfitting. Overfitting is when the model is too complex and does well on the training data but not on the test data. Underfitting is when the model is too simple and performs poorly on the training and testing data sets. \n","\n","Overfitting is simple to deal with, using methods like regularization, which we will discuss in the next lab. To deal with underfitting, we can build a more complex model using methods like polynomial regression. If making a more complex model does not work, this may involve using more data to train the model on or obtaining new features. As this process is complex, it's better to determine if the model can overfit the data first. Therefore, in this section, we will use Polynomial Regression to overfit the data to determine if we have an adequate amount of data.\n","\n","In this notebook, we will explore Polynomial Regression and perform polynomial transform using individual features as well as multiple features.\n","\n","\n","## Objectives\n","\n","After completing this lab you will be able to:\n","\n","*   Understand the concept of  overfitting versus underfitting\n","*   Apply polynomial transforms to data \n","*   Perform  hyperparameters grid search on a model, using validation data \n","\n"]},{"cell_type":"markdown","id":"dfe83524-f957-4df2-82e4-9c0bb879a6c9","metadata":{},"outputs":[],"source":["***\n"]},{"cell_type":"markdown","id":"138d252e-af8c-4381-bdeb-a6575164e552","metadata":{},"outputs":[],"source":["## **Setup**\n"]},{"cell_type":"markdown","id":"3f4ef468-75e7-45ea-bf7b-502a9b5dfbe0","metadata":{},"outputs":[],"source":["For this lab, we will be using the following libraries:\n"," - [`pandas`](https://pandas.pydata.org/) for managing the data.\n"," - [`numpy`](https://numpy.org/) for mathematical operations.\n"," - [`seaborn`](https://seaborn.pydata.org/) for visualizing the data.\n"," - [`matplotlib`](https://matplotlib.org/) for visualizing the data.\n"," - [`sklearn`](https://scikit-learn.org/stable/) for machine learning and machine-learning-pipeline related functions.\n"," - [`scipy`](https://docs.scipy.org/doc/scipy/tutorial/stats.html/) for statistical computations.\n"]},{"cell_type":"markdown","id":"f1f23ad9-c750-40d8-bae7-5ec8ce9d9877","metadata":{},"outputs":[],"source":["## **Import the required libraries**\n"]},{"cell_type":"markdown","id":"8a085292-721d-4604-871b-ffbeffb754a2","metadata":{},"outputs":[],"source":["The following required modules are pre-installed in the Skills Network Labs environment. However, if you run this notebook commands in a different Jupyter environment (e.g. Watson Studio or Ananconda), you will need to install these libraries by removing the `#` sign before `!mamba` in the code cell below.\n"]},{"cell_type":"code","id":"8193ad54-3adc-4a10-9699-c226c2e3394f","metadata":{},"outputs":[],"source":["# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n# !mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\n# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\""]},{"cell_type":"code","id":"9c878de0-20f0-407d-b3c0-9ad091ee02fc","metadata":{},"outputs":[],"source":["#!pip install -U scikit-learn"]},{"cell_type":"code","id":"65e1fe30-07af-43e7-b627-5647dbe76db4","metadata":{},"outputs":[],"source":["# Surpress warnings:\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\n"]},{"cell_type":"code","id":"0b109104-09b8-499d-b00b-7b2e25a8bb56","metadata":{},"outputs":[],"source":["import pandas as pd\nimport numpy as np \n\nimport seaborn as sns \nimport matplotlib.pylab as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score \nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV"]},{"cell_type":"markdown","id":"808d9566-fc28-407f-9d0d-d215dc4b1660","metadata":{},"outputs":[],"source":["The function below will calculate the $R^{2}$ on each feature given the a input model.\n"]},{"cell_type":"code","id":"07725ecb-1e74-4163-8362-d14989c05450","metadata":{},"outputs":[],"source":["def get_R2_features(model,test=True): \n    #X: global  \n    features=list(X)\n    features.remove(\"three\")\n    \n    R_2_train=[]\n    R_2_test=[]\n\n    for feature in features:\n        model.fit(X_train[[feature]],y_train)\n        \n        R_2_test.append(model.score(X_test[[feature]],y_test))\n        R_2_train.append(model.score(X_train[[feature]],y_train))\n        \n    plt.bar(features,R_2_train,label=\"Train\")\n    plt.bar(features,R_2_test,label=\"Test\")\n    plt.xticks(rotation=90)\n    plt.ylabel(\"$R^2$\")\n    plt.legend()\n    plt.show()\n    print(\"Training R^2 mean value {} Testing R^2 mean value {} \".format(str(np.mean(R_2_train)),str(np.mean(R_2_test))) )\n    print(\"Training R^2 max value {} Testing R^2 max value {} \".format(str(np.max(R_2_train)),str(np.max(R_2_test))) )"]},{"cell_type":"markdown","id":"debe52cb-3c7c-46b8-a143-04d7d25c3521","metadata":{},"outputs":[],"source":["The function below will plot the distribution of two inputs.\n"]},{"cell_type":"code","id":"d26fd742-f1b3-4ffd-a81f-4cae29234647","metadata":{},"outputs":[],"source":["def  plot_dis(y,yhat):\n    \n    plt.figure()\n    ax1 = sns.distplot(y, hist=False, color=\"r\", label=\"Actual Value\")\n    sns.distplot(yhat, hist=False, color=\"b\", label=\"Fitted Values\" , ax=ax1)\n    plt.legend()\n\n    plt.title('Actual vs Fitted Values')\n    plt.xlabel('Price (in dollars)')\n    plt.ylabel('Proportion of Cars')\n\n    plt.show()\n    plt.close()"]},{"cell_type":"markdown","id":"9ac2ceac-5856-41b0-91a5-ca7ad5eb7d38","metadata":{},"outputs":[],"source":["## **Reading and understanding our data**\n"]},{"cell_type":"markdown","id":"c75ad191-c8de-40c8-af90-671dcb7cc17e","metadata":{},"outputs":[],"source":["For this lab, we will be using the car sales dataset, hosted on IBM Cloud object storage. This dataset can also be found and downloaded from [kaggle.com](https://www.kaggle.com/datasets/goyalshalini93/car-data), an open public data source.\n","The dataset contains all the information about cars, a name of a manufacturer, all car's technical parameters and a sale price of a car.\n","\n","This dataset has already been pre-cleaned and encoded (using one-hot and label encoders) in the Linear Regression Notebook.\n"]},{"cell_type":"markdown","id":"5a16d466-b1db-4ed3-99f7-0daeb59fa18a","metadata":{},"outputs":[],"source":["Let's read the data into *pandas* data frame and look at the first 5 rows using the `head()` method.\n"]},{"cell_type":"code","id":"a979c663-1be8-4ffb-acfb-44fdbb7f514a","metadata":{},"outputs":[],"source":["data = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML240EN-SkillsNetwork/labs/encoded_car_data.csv')\ndata.head()"]},{"cell_type":"markdown","id":"d5e69571-4223-45dd-86a7-78559c011118","metadata":{},"outputs":[],"source":["We can find more information about the features and types using the `info()`  method.\n"]},{"cell_type":"code","id":"cbe14862-a8e0-40f0-97a3-a559d731592b","metadata":{},"outputs":[],"source":["data.info()"]},{"cell_type":"markdown","id":"7db24710-ef51-41f3-8e18-c97b8b0d224a","metadata":{},"outputs":[],"source":["We have 35 features in our dataset after the one hot encoding. \n","\n","Before we begin our polynomial analysis, let's visualize some of the relationships between our features and the target variable, 'price'.\n"]},{"cell_type":"code","id":"568f9403-e9c1-4867-8857-b741a4ccefa6","metadata":{},"outputs":[],"source":["sns.lmplot(x = 'curbweight', y = 'price', data = data, order=2)"]},{"cell_type":"code","id":"6dd22746-7a46-47be-8725-f2cbe2b154f3","metadata":{},"outputs":[],"source":["sns.lmplot(x = 'carlength', y = 'price', data = data, order=2)"]},{"cell_type":"markdown","id":"afa4e39a-269d-4cd7-8c86-338ca7030885","metadata":{},"outputs":[],"source":["The relationship is more curved.\n"]},{"cell_type":"markdown","id":"a23015ae-45bd-487d-91fb-745308d89742","metadata":{},"outputs":[],"source":["## Exercise 1\n","In this Exercise, visualize the relationship between the 'horsepower' and the target variable, 'price'.\n"]},{"cell_type":"code","id":"b5431d62-f4a5-4a4a-9917-f61ca54f8e1c","metadata":{},"outputs":[],"source":["# Enter your code and run the cell\n"]},{"cell_type":"markdown","id":"a2fa3ecf-3c55-44d7-be20-ff73490b1542","metadata":{},"outputs":[],"source":["<details>\n","<summary><strong>Solution</strong> (Click Here)</summary>\n","```python\n","    \n","sns.lmplot(x = 'horsepower', y = 'price', data = data, order=2)\n","    \n","```\n","</details>\n"]},{"cell_type":"markdown","id":"c13f4652-84b9-43bf-9360-29640a0c1e8e","metadata":{},"outputs":[],"source":["## Data Preparation\n","\n","Let's first split our data into ```X``` features and ```y``` target.\n"]},{"cell_type":"code","id":"b52b8703-7610-4415-a2fe-1d13f1f421a9","metadata":{},"outputs":[],"source":["X = data.drop('price', axis=1)\ny = data.price"]},{"cell_type":"markdown","id":"36e6e7e1-587b-47ea-a235-628d637b8968","metadata":{},"outputs":[],"source":["## Train Test Split \n"]},{"cell_type":"markdown","id":"7569c342-6394-4b71-b27c-53e02022dbe1","metadata":{},"outputs":[],"source":["Now that we have split our data into training and testing sets, the training data is used for your model to recognize patterns using some criteria,the test data set it used to evaluate your model, as shown in the following image:\n"]},{"cell_type":"markdown","id":"eeba8f6f-4e3f-4a78-b353-c943839ed0a3","metadata":{},"outputs":[],"source":["<center>\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML240EN-SkillsNetwork/images/trin-test.png\">\n","</center>\n","<center>source scikit-learn.org</center>\n"]},{"cell_type":"markdown","id":"e3667786-0255-4576-a1b4-006b018487d4","metadata":{},"outputs":[],"source":["Now, we split our data, using <code>train_test_split</code> function, into the training and testing sets, allocating 30% of the data for testing.\n"]},{"cell_type":"code","id":"47a10b16-4c4b-4813-b5cc-750bbdfd8f7a","metadata":{},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)\nprint(\"Number of test samples:\", X_test.shape[0])\nprint(\"Number of training samples:\", X_train.shape[0])\n"]},{"cell_type":"markdown","id":"9560ab89-bbdb-4117-94f4-e1701ce568b1","metadata":{},"outputs":[],"source":["### Multiple Features\n"]},{"cell_type":"markdown","id":"6f931f19-722c-4862-836e-1d1b699deda5","metadata":{},"outputs":[],"source":["Let's create a <code>LinearRegression</code> object, called `lm`. \n"]},{"cell_type":"code","id":"a77505c3-cca9-4f52-ab56-b63d5a89f708","metadata":{},"outputs":[],"source":["lm = LinearRegression()"]},{"cell_type":"markdown","id":"96886507-7810-4bed-a606-a730203414a9","metadata":{},"outputs":[],"source":["Now, let's fit the model with multiple features on our X_train and y_train data.\n"]},{"cell_type":"code","id":"d1aa700e-325c-400b-85dd-68e345a7f0c3","metadata":{},"outputs":[],"source":["lm.fit(X_train, y_train)"]},{"cell_type":"markdown","id":"3c4c9fc4-2b8f-4157-b97a-6db2539a12d0","metadata":{},"outputs":[],"source":["We apply `predict(`) function on the testing data set.\n"]},{"cell_type":"code","id":"efcd2188-261b-4b0e-b137-8e004cf1c8a4","metadata":{},"outputs":[],"source":["predicted = lm.predict(X_test)"]},{"cell_type":"markdown","id":"5712a4e7-8880-4ca8-a14d-e9ffb4890ee5","metadata":{},"outputs":[],"source":["Let's calculate the `r2_score()` on both, training and testing data sets.\n"]},{"cell_type":"code","id":"6d45a177-6b49-4073-a21a-9f56cbbb1203","metadata":{},"outputs":[],"source":["print(\"R^2 on training  data \",lm.score(X_train, y_train))\nprint(\"R^2 on testing data \",lm.score(X_test,y_test))"]},{"cell_type":"markdown","id":"2b89e15f-931f-4af5-846e-678fb96513f2","metadata":{},"outputs":[],"source":["We can plot distributions of the predicted values versus the actual values. \n"]},{"cell_type":"code","id":"14df8417-914d-4b49-9bbb-a32c519c0c56","metadata":{},"outputs":[],"source":["plot_dis(y_test,predicted)"]},{"cell_type":"markdown","id":"9c912a8e-36e0-4233-b596-b5bba5a3dd40","metadata":{},"outputs":[],"source":["Below, we will view the estimated coefficients for the linear regression problem.\n"]},{"cell_type":"code","id":"50802121-bbd1-404c-b147-442f9171d039","metadata":{},"outputs":[],"source":["{col:coef for col,coef in zip(X.columns, lm.coef_)}"]},{"cell_type":"markdown","id":"6113d19c-11b5-4d5f-9945-d80e19f756e6","metadata":{},"outputs":[],"source":["As we see, the first two coefficients are too large to plot, so we'll drop them and plot the rest of the coefficients.\n"]},{"cell_type":"code","id":"61307b7f-0038-4c9f-a84a-04dd4c45b71d","metadata":{},"outputs":[],"source":["plt.bar(X.columns[2:],abs(lm.coef_[2:]))\nplt.xticks(rotation=90)\nplt.ylabel(\"$coefficients$\")\nplt.show()"]},{"cell_type":"markdown","id":"328793d3-b4d6-400e-91b3-536126958b9f","metadata":{},"outputs":[],"source":["Usually, we can interpret the lager coefficients as having more importance on the prediction, but this is not always the case, so let's look at the individual features. \n"]},{"cell_type":"markdown","id":"4650e3d8-db8c-48d1-89ba-942f34493d03","metadata":{},"outputs":[],"source":["### Individual Features \n"]},{"cell_type":"markdown","id":"65908dae-3eb2-4480-afde-5f51ca382d68","metadata":{},"outputs":[],"source":["We can train the model and plot our $R^2$ for each of the features on the training and testing data sets, using the function <code>get_R2_features</code>.\n"]},{"cell_type":"code","id":"97cbc2fc-0f08-49b9-ac11-ca2c54798d0e","metadata":{},"outputs":[],"source":["get_R2_features(lm)"]},{"cell_type":"markdown","id":"31f562b7-cc2c-490a-bbe7-b4547a28e7fd","metadata":{},"outputs":[],"source":["From the above plot, we see that some individual features perform similarly to  using all the features (we removed the feature ```three``` ), in  addition, we see that smaller coefficients seem to correspond to a larger $R^{2}$, therefore  larger coefficients correspond to overfiting.\n"]},{"cell_type":"markdown","id":"6cc639a7-a8a3-4814-a51d-faad5dc2527d","metadata":{},"outputs":[],"source":["## Exercise 2\n","In this Exercise, calculate the $R^2$ using the object Pipeline for  Linear Regression and apply ```StandardScaler()``` to all features, then use the function ```plot_dis``` to compare the predicted values versus the actual values.\n"]},{"cell_type":"code","id":"889812c5-880e-42b5-9da6-fd3f163b17eb","metadata":{},"outputs":[],"source":["# Enter your code and run the cell\n"]},{"cell_type":"markdown","id":"b30aba7d-5b84-4921-8b17-9531029c5379","metadata":{},"outputs":[],"source":["<details>\n","<summary><strong>Solution</strong> (Click Here)</summary>\n"," ```python\n","    \n","pipe = Pipeline([('ss',StandardScaler() ),('lr', LinearRegression())])\n","pipe.fit(X_train,y_train)\n","print(\"R^2 on training  data \", pipe.score(X_train, y_train))\n","print(\"R^2 on testing data \", pipe.score(X_test,y_test))\n","predicted = pipe.predict(X_test)\n","plot_dis(y_test,predicted)\n","    \n","```\n","</details>\n"]},{"cell_type":"markdown","id":"49d5b930-5266-4993-9a90-4de05d36bc38","metadata":{},"outputs":[],"source":["## Exercise 3\n","In this Exercise, calculate the $R^2$ using the object Pipeline with  ```StandardScaler()``` for each individual features using the function ```get_R2_features```.\n"]},{"cell_type":"code","id":"2d36ec40-11a8-4a07-ba21-41888072a1d2","metadata":{},"outputs":[],"source":["# Enter your code and run the cell\n"]},{"cell_type":"markdown","id":"abc6e452-0a35-4bf2-9c5d-c0ad44a2e77c","metadata":{},"outputs":[],"source":["<details>\n","<summary><strong>Solution</strong> (Click Here)</summary>\n","```python\n","    \n","pipe = Pipeline([('ss',StandardScaler() ),('lr', LinearRegression())])\n","get_R2_features(pipe)\n","    \n","```\n","</details>\n"]},{"cell_type":"markdown","id":"39cffe31-91e3-4413-be65-6d324f05d569","metadata":{},"outputs":[],"source":["## Polynomial Features\n"]},{"cell_type":"markdown","id":"effd5500-e91e-4e3c-91fb-95aea57ede81","metadata":{},"outputs":[],"source":["### Multiple Features\n"]},{"cell_type":"markdown","id":"79727a38-9433-4f08-b10c-3dbb62ba2625","metadata":{},"outputs":[],"source":["Polynomial transform is a simple way to increase the complexity of the model, but we must be mindful of overfilling.\n","Below, we will perform a second degree (degree=2) polynomial transformation.\n"]},{"cell_type":"code","id":"0695626e-9216-4504-9724-bebb1bcfbe88","metadata":{},"outputs":[],"source":["poly_features = PolynomialFeatures(degree=2, include_bias=False)"]},{"cell_type":"markdown","id":"84778ecc-cf0d-46a7-965a-c0c037d95bdb","metadata":{},"outputs":[],"source":["Now, we transform the training and testing data sets.\n"]},{"cell_type":"code","id":"84471bdc-34df-482b-80e7-55271258c521","metadata":{},"outputs":[],"source":["X_train_poly = poly_features.fit_transform(X_train)\nX_test_poly = poly_features.transform(X_test)"]},{"cell_type":"markdown","id":"aeb0ee3d-fd2f-477b-8655-48da15a950cc","metadata":{},"outputs":[],"source":["`X_train_poly` and `X_test_poly` now contain the original features of X plus the square of these features and the cross-terms combination.\n","Let's check the shape of the newly created train and test sets.\n"]},{"cell_type":"code","id":"0f2de66d-e0a0-4c42-af3c-54c8ef3b9b25","metadata":{},"outputs":[],"source":["print(X_train_poly.shape)\n"]},{"cell_type":"code","id":"ebc8cc05-ef6e-47a9-aba0-e6e21ac1e18c","metadata":{},"outputs":[],"source":["print(X_test_poly.shape)"]},{"cell_type":"markdown","id":"0cef8c64-bb3a-4264-b6fc-5de44b09f827","metadata":{},"outputs":[],"source":["Altogether, we have 665 features. Now, we fit the model with the newly created features.\n"]},{"cell_type":"code","id":"d0299d8b-349c-4fbc-be40-4248c6a5b12b","metadata":{},"outputs":[],"source":["lm = LinearRegression()\nlm.fit(X_train_poly, y_train)"]},{"cell_type":"markdown","id":"a8cb4ece-52e4-4ab6-9ed8-1d508371dcec","metadata":{},"outputs":[],"source":["And we make predictions.\n"]},{"cell_type":"code","id":"4463e0f2-028e-469d-9b83-11b89828a181","metadata":{},"outputs":[],"source":["predicted = lm.predict(X_train_poly)"]},{"cell_type":"markdown","id":"e4934ca0-9ce6-4bbc-a0a2-8ecdc42e0c16","metadata":{},"outputs":[],"source":["Again, we can ckeck the `r2_score()` on both, training and testing data sets.\n"]},{"cell_type":"code","id":"41f38919-abdf-4b4c-a2ca-8d41bde11126","metadata":{},"outputs":[],"source":["print(\"R^2 on training data:\", lm.score(X_train_poly, y_train))\nprint(\"R^2 on testing data:\", lm.score(X_test_poly,y_test))"]},{"cell_type":"markdown","id":"2c9c2074-ae54-4783-962e-95cbe030afa4","metadata":{},"outputs":[],"source":["We see the model has a negative $R^{2}$ on the test data set, this is sign of overfiting.\n"]},{"cell_type":"markdown","id":"a7b336bf-b899-43a3-b1c8-73bdcce8df08","metadata":{},"outputs":[],"source":["### Individual Features \n"]},{"cell_type":"markdown","id":"6daa316d-6af1-41b4-b0e5-0c3db63248f1","metadata":{},"outputs":[],"source":["<p>Data Pipelines simplify the steps of processing the data. We use the module <code>Pipeline</code> to create a pipeline. We also use <code>PolynomialFeatures</code> as a step in our pipeline.</p>\n"]},{"cell_type":"code","id":"60e737cc-35ef-446a-b005-9ef2c29ab2c4","metadata":{},"outputs":[],"source":["Input=[ ('polynomial', PolynomialFeatures(include_bias=False,degree=2)), ('model', LinearRegression())]"]},{"cell_type":"markdown","id":"45981a66-e2d9-478d-8736-ac124c214c18","metadata":{},"outputs":[],"source":["We can repeat the steps above, using the <code>Pipleine</code> object.\n"]},{"cell_type":"code","id":"95473bd4-5e9a-41ec-9208-1d8dffb32195","metadata":{},"outputs":[],"source":["pipe=Pipeline(Input)\npipe.fit(X_train, y_train)"]},{"cell_type":"markdown","id":"23ebcb8b-97e5-4b56-b066-8a6ae1f5e5da","metadata":{},"outputs":[],"source":["We can see the results are identical.\n"]},{"cell_type":"code","id":"3540199c-f484-40ca-9d57-8aa1638e1359","metadata":{},"outputs":[],"source":["print(\"R^2 on training  data:\", pipe.score(X_train, y_train))\nprint(\"R^2 on testing data:\", pipe.score(X_test,y_test))"]},{"cell_type":"markdown","id":"b0730f21-bbfd-45fa-9108-77d965f2755a","metadata":{},"outputs":[],"source":["We can train our model on each of the features using the Polynomial Feature transform of the second degree. Then we can plot our $R^2$. \n"]},{"cell_type":"code","id":"c894af95-8f3e-43da-ac6d-1dee62e6414a","metadata":{},"outputs":[],"source":["get_R2_features(pipe)"]},{"cell_type":"markdown","id":"ee8a4ed9-bd51-4b61-a104-19cf86eeab55","metadata":{},"outputs":[],"source":["Feature with the max $R^{2}$ is higher than when using all the features.\n"]},{"cell_type":"markdown","id":"104f2786-f9f9-444f-bc76-c685819eb634","metadata":{},"outputs":[],"source":["## GridSearch and  Pipeline\n"]},{"cell_type":"markdown","id":"edd4dff9-81ac-4e73-a7df-1781efd608ff","metadata":{},"outputs":[],"source":["In this section of the notebook, we will define a pipeline object, then use GridSearch to find the best hyper-parameters of the model by using  cross-validation method of the  parameter grid, as shown in the figure below. A 5-fold cross validation is used by default. We will learn more about k-fold cross validation in the next, Cross Validation lesson of the Course.\n"]},{"cell_type":"markdown","id":"d499cb0f-629b-416b-992b-6848db886ffe","metadata":{},"outputs":[],"source":["<center>\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML240EN-SkillsNetwork/images/k-fold.png\">\n","</center>\n","<center>source scikit-learn.org</center>\n"]},{"cell_type":"markdown","id":"9e7b3bc9-4690-43c3-ba2f-b515a3b0a13a","metadata":{},"outputs":[],"source":["We create `PolynomialFeatures()` pipeline.\n"]},{"cell_type":"code","id":"9b66c761-8f60-4772-ab8d-ca376c6253be","metadata":{},"outputs":[],"source":["Input=[ ('polynomial', PolynomialFeatures(include_bias=False,degree=2)), ('model',LinearRegression())]\npipe=Pipeline(Input)"]},{"cell_type":"markdown","id":"8e335364-c42b-438b-944a-66ed2bc6f32c","metadata":{},"outputs":[],"source":["To search for the best combination of hyperparameters, we create a  <b>GridSearchCV</b> object with dictionary of parameter values. The parameters of pipelines can be set using the name of the key separated by \n","'__', then the parameter name of the key.\n"]},{"cell_type":"code","id":"45c798d5-44e8-4bad-822c-652d5a63fbdc","metadata":{},"outputs":[],"source":["param_grid = {\n    \"polynomial__degree\": [1, 2, 3],\n    \"model__normalize\":[True, False]\n    \n}"]},{"cell_type":"markdown","id":"75ce357b-80cf-4735-9540-26b1d0456948","metadata":{},"outputs":[],"source":["<b>polynomial__degree</b>: is the degree of the polynomial. \n","\n","<b>model__normalize</b> This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm.\n"]},{"cell_type":"markdown","id":"caa9a861-3404-4a3d-b398-ec85ea729ad8","metadata":{},"outputs":[],"source":["The model is overfitting.\n"]},{"cell_type":"code","id":"3619696a-bc61-46fb-8221-d00eb08fc1a7","metadata":{},"outputs":[],"source":["search = GridSearchCV(pipe, param_grid, n_jobs=1)\n"]},{"cell_type":"code","id":"00822fe6-0653-4367-bc2c-bcb934ce32c3","metadata":{},"outputs":[],"source":["pipe.fit(X_train, y_train)"]},{"cell_type":"code","id":"96dbe961-f21c-4136-8458-19605faf65d6","metadata":{},"outputs":[],"source":["search.fit(X_test, y_test)"]},{"cell_type":"markdown","id":"e23f4710-a9b0-48f5-9b35-c2fe4d812999","metadata":{},"outputs":[],"source":["The object finds the best parameter values on the validation data. We can obtain the estimator with the best parameters and assign it to the variable ```best```, as follows:\n"]},{"cell_type":"code","id":"75ca01c2-1894-448d-9ff1-280bf6f3c9ec","metadata":{},"outputs":[],"source":["best=search.best_estimator_\nbest"]},{"cell_type":"markdown","id":"2b5b99ce-5297-439d-8a1c-750319738b84","metadata":{},"outputs":[],"source":["We see the degree is one and normalize is <code>Ture </code>.\n"]},{"cell_type":"markdown","id":"ce73f881-166f-4b61-b7de-63553d93f704","metadata":{},"outputs":[],"source":["Now, we test our model on the test data to see the best score.\n"]},{"cell_type":"code","id":"6d915e2e-2601-4be5-a489-3a6b2dcec436","metadata":{},"outputs":[],"source":["best.score(X_test,y_test)"]},{"cell_type":"markdown","id":"c023cb83-72fc-487d-b9e4-28e9d8728ad9","metadata":{},"outputs":[],"source":["Finally, we can plot a distribution of the predicted values versus the actual values. \n"]},{"cell_type":"code","id":"297d90bc-b7f2-49eb-80c0-91fb85bb6b7e","metadata":{},"outputs":[],"source":["predicted=best.predict(X_test)\nplot_dis(y_test,predicted)"]},{"cell_type":"markdown","id":"15bc2bc9-d6b6-4d12-9b3c-e1b24b256442","metadata":{},"outputs":[],"source":["As we see, the result is almost perfect!\n"]},{"cell_type":"markdown","id":"02563916-1972-48d8-8eee-a48c1d07c1cf","metadata":{},"outputs":[],"source":["## Exercise 4\n","In this Exercise, calculate the $R^2$ using the object ```Pipeline``` with ```GridSearch``` for each individual features.\n"]},{"cell_type":"code","id":"717fcbe6-8abb-4b38-82d2-95882f528a68","metadata":{},"outputs":[],"source":["# Enter your code and run the cell\n"]},{"cell_type":"markdown","id":"64eaac1e-9941-44e4-91aa-d107cbf345a9","metadata":{},"outputs":[],"source":["<details>\n","<summary><strong>Solution</strong> (Click Here)</summary>\n","\n","```python    \n","features=list(X)\n","   \n","    \n","R_2_train=[]\n","R_2_test=[]\n","\n","for feature in features:\n","    param_grid = {\n","    \"polynomial__degree\": [ 1, 2,3,4,5],\n","    \"model__positive\":[True, False]}\n","    Input=[ ('polynomial', PolynomialFeatures(include_bias=False,degree=2)), ('model',LinearRegression())]\n","    pipe=Pipeline(Input)\n","    print(feature)\n","    search = GridSearchCV(pipe, param_grid, n_jobs=2)\n","    search.fit(X_test[[feature]], y_test)\n","    best=search.best_estimator_\n","        \n","    R_2_test.append(best.score(X_test[[feature]],y_test))\n","    R_2_train.append(best.score(X_train[[feature]],y_train))\n","    \n","        \n","plt.bar(features,R_2_train,label=\"Train\")\n","plt.bar(features,R_2_test,label=\"Test\")\n","plt.xticks(rotation=90)\n","plt.ylabel(\"$R^2$\")\n","plt.legend()\n","plt.show()\n","print(\"Training R^2 mean value {} Testing R^2 mean value {} \".format(str(np.mean(R_2_train)),str(np.mean(R_2_test))) )\n","print(\"Training R^2 max value {} Testing R^2 max value {} \".format(str(np.max(R_2_train)),str(np.max(R_2_test))) )\n","```\n","</details>\n"]},{"cell_type":"markdown","id":"545e70cf-0a81-4708-ad98-0185b49e3bc9","metadata":{},"outputs":[],"source":["# Congratulations! - You have completed the lab\n"]},{"cell_type":"markdown","id":"94ee4d9a-ac41-4e7b-bfdd-25752993fae7","metadata":{},"outputs":[],"source":["## Authors\n"]},{"cell_type":"markdown","id":"9101b6d5-50da-4260-af54-eb8cd62930bc","metadata":{},"outputs":[],"source":["<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2021-01-01\" target=\"_blank\">Joseph Santarcangelo</a>\n","\n","[Svitlana Kramar](www.linkedin.com/in/svitlana-kramar)\n"]},{"cell_type":"markdown","id":"5baa38ec-5bb2-4391-9f00-2fc09111cbc3","metadata":{},"outputs":[],"source":["<!--## Change Log\n","-->\n"]},{"cell_type":"markdown","id":"a63292a4-c497-41cc-a89c-0c7b1f91f8ae","metadata":{},"outputs":[],"source":["<!--| Date (YYYY-MM-DD) | Version | Changed By            | Change Description                   |\n","| ----------------- | ------- | --------------------- | ------------------------------------ |\n","|   2022-04-15      | 0.1     | Svitlana K.           | Created the first draft |\n","|   2022-04-18      | 0.1     | Joseph S.             | Updated all content |\n","|   2022-04-26      | 0.1     | Svitlana K.           | Corrected minor grammar errors |-->\n","\n"]},{"cell_type":"code","id":"88edd5e6-69bc-4e09-a01e-096d22074822","metadata":{},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"prev_pub_hash":"bec921a2df95c8e7621f3c071955d3edd491aed478a390763a258e2f1c4e4d9e"},"nbformat":4,"nbformat_minor":4}