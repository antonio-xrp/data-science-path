{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Resumen/Revisión\n",
    "\n",
    "### Técnicas de regularización\n",
    "Las tres fuentes de error de su modelo son: el sesgo, la varianza y, el error irreducible.\n",
    "\n",
    "La regularización es una forma de conseguir construir modelos sencillos con un error relativamente bajo. Le ayuda a evitar el sobreajuste penalizando los coeficientes de alto valor. Reduce los parámetros y encoge el modelo.\n",
    "\n",
    "La regularización añade un parámetro ajustable de fuerza de regularización directamente en la función de costes.\n",
    "\n",
    "La regularización realiza la selección de características reduciendo la contribución de las mismas, lo que puede evitar el sobreajuste.\n",
    "\n",
    "En la regresión Ridge, la penalización de complejidad λ se aplica proporcionalmente a los valores de los coeficientes al cuadrado.\n",
    "\n",
    "- El término de penalización tiene el efecto de \"encoger\" los coeficientes hacia 0.\n",
    "\n",
    "- Esto impone un sesgo al modelo, pero también reduce la varianza.\n",
    "\n",
    "- Podemos seleccionar la mejor fuerza de regularización lambda mediante validación cruzada.\n",
    "\n",
    "- Es una buena práctica escalar las características (es decir, utilizando StandardScaler) para que las penalizaciones no se vean afectadas por la escala de las variables.\n",
    "\n",
    "En la regresión LASSO: la penalización de complejidad λ (lambda) es proporcional al valor absoluto de los coeficientes. LASSO son las siglas de : Operador de Selección y Reducción Mínima Absoluta.\n",
    "\n",
    "- Efecto similar al de Ridge en términos de compromiso de complejidad: aumentar lambda aumenta el sesgo pero disminuye la varianza.\n",
    "\n",
    "- LASSO tiene más probabilidades que Ridge de realizar la selección de características, en el sentido de que para una λ fija, LASSO tiene más probabilidades de que los coeficientes se fijen en cero.\n",
    "\n",
    "La red elástica combina las penalizaciones de la regresión Ridge y LASSO. Requiere el ajuste de un parámetro adicional que determina el énfasis de las penalizaciones de regularización L1 frente a L2.\n",
    "\n",
    "La propiedad de selección de características de LASSO proporciona una ventaja de interpretabilidad, pero puede tener un rendimiento inferior si el objetivo depende realmente de muchas de las características.\n",
    "\n",
    "Elastic Net, un enfoque híbrido alternativo, introduce un nuevo parámetro α (alfa) que determina una media ponderada de las penalizaciones L1 y L2.\n",
    "\n",
    "Las técnicas de regularización tienen una interpretación analítica, geométric"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
