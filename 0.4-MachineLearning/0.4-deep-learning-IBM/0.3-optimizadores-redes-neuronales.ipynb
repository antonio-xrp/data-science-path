{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers and momentum\n",
    "\n",
    "momentum en la optimización de redes neuronales.\n",
    "\n",
    "El momentum es como un patinador que, al empujar con fuerza, sigue deslizándose en la misma dirección. En el contexto de la optimización, cuando estamos ajustando los pesos de nuestra red neuronal, a veces los cambios pueden ser pequeños y desiguales. Con el momentum, en lugar de hacer ajustes pequeños y erráticos, tomamos en cuenta los pasos anteriores para hacer movimientos más suaves y consistentes hacia el objetivo. Esto significa que si hemos estado moviéndonos en una dirección, el momentum nos ayuda a seguir en esa dirección, haciendo que los pasos sean más grandes y menos fluctuantes.\n",
    "\n",
    "Imagina que estás en una montaña y quieres llegar a la cima. Si solo miras hacia abajo en cada paso, podrías desviarte. Pero si recuerdas el camino que ya has recorrido, puedes hacer ajustes más inteligentes y llegar más rápido a la cima. Así es como el momentum ayuda a las redes neuronales a aprender de manera más eficiente.\n",
    "\n",
    "**Introducción a los optimizadores**\n",
    "\n",
    "- Los optimizadores son métodos que mejoran el proceso de ajuste de los pesos en una red neuronal.\n",
    "- Existen diferentes enfoques de descenso de gradiente: estocástico, mini-batch y batch completo.\n",
    "\n",
    "**Momentum**\n",
    "\n",
    "- El momentum suaviza el proceso de ajuste de pesos al considerar pasos anteriores, lo que permite movimientos más consistentes y menos erráticos.\n",
    "- Se utiliza un parámetro de momentum (n) que, generalmente, se establece en 0.9 para lograr un buen equilibrio entre suavidad y velocidad.\n",
    "\n",
    "**Nesterov Momentum**\n",
    "\n",
    "- Este método mejora el momentum al anticipar el siguiente paso, lo que ayuda a evitar el sobreajuste y a hacer ajustes más precisos.\n",
    "- Se calcula el gradiente teniendo en cuenta la dirección del momentum, lo que permite pasos más grandes y precisos hacia el óptimo.\n",
    "\n",
    "Recuerda que estos métodos son herramientas valiosas para optimizar el aprendizaje de las redes neuronales. Si tienes más preguntas o necesitas más detalles, ¡estoy aquí para ayudarte!\n",
    "\n",
    "---\n",
    "### Regularization techniques for deep learning\n",
    "\n",
    "Técnicas de regularización\n",
    "\n",
    "- La regularización es una modificación en el algoritmo de aprendizaje que busca reducir el error de generalización, a menudo a expensas del error de entrenamiento.\n",
    "- Se pueden utilizar penalizaciones en la función de costo para evitar que los pesos sean demasiado altos, similar a la regresión ridge.\n",
    "\n",
    "**Dropout**\n",
    "\n",
    "- El dropout consiste en eliminar aleatoriamente un subconjunto de neuronas durante el entrenamiento, lo que evita que el modelo dependa demasiado de rutas específicas.\n",
    "- Al final del entrenamiento, se deben reescalar los pesos de las neuronas para reflejar el porcentaje de tiempo que estuvieron activas.\n",
    "\n",
    "**Detención temprana**\n",
    "\n",
    "- La detención temprana implica parar el entrenamiento si la pérdida en un conjunto de validación comienza a aumentar, evitando así el sobreajuste.\n",
    "- Se recomienda verificar la pérdida de validación cada ciertos epochs para decidir si continuar o detener el entrenamiento.\n",
    "\n",
    "\n",
    "--- \n",
    "### Popular optimizaers\n",
    "\n",
    "Este material se centra en los optimizadores AdaGrad, RMSProp y Adam, que son fundamentales para mejorar el proceso de optimización en el aprendizaje profundo.\n",
    "\n",
    "**AdaGrad**\n",
    "\n",
    "- Escala las actualizaciones de cada peso de manera individual, reduciendo la tasa de aprendizaje para pesos que se actualizan con frecuencia.\n",
    "- Utiliza una suma acumulativa de los cuadrados de los gradientes, lo que provoca que la tasa de aprendizaje disminuya con el tiempo.\n",
    "\n",
    "**RMSProp**\n",
    "\n",
    "- Similar a AdaGrad, pero da más peso a los gradientes recientes al decaer los más antiguos.\n",
    "- Mejora la eficiencia de las actualizaciones al adaptarse a los gradientes recientes, lo que lo hace más efectivo que AdaGrad.\n",
    "\n",
    "**Adam**\n",
    "\n",
    "- Combina las ideas de momentum y RMSProp, utilizando términos específicos para cada uno.\n",
    "- Se corrige el sesgo en las estimaciones iniciales y se ajusta la tasa de aprendizaje para optimizar el proceso de convergencia.\n",
    "\n",
    "Recuerda que la elección del optimizador puede depender del problema específico que estés abordando. ¡Sigue practicando y no dudes en preguntar si necesitas más aclaraciones!\n",
    "\n",
    "### Details of Training Neural Networks\n",
    "\n",
    "Este material se centra en los aspectos clave del entrenamiento de modelos de redes neuronales, incluyendo diferentes enfoques de descenso de gradiente y la importancia de ciertos parámetros.\n",
    "\n",
    "Enfoques de descenso de gradiente\n",
    "\n",
    "- **Descenso de gradiente por lotes completos:** Utiliza todo el conjunto de datos para calcular el gradiente antes de actualizar los pesos. Es más preciso pero más lento.\n",
    "- **Descenso de gradiente estocástico (SGD):** Calcula el gradiente usando una sola fila de datos, lo que permite pasos más rápidos pero menos informados.\n",
    "\n",
    "Compromiso con el descenso de gradiente por mini-lotes\n",
    "\n",
    "- **Mini-batch gradient descent:** Utiliza un subconjunto del conjunto de datos (típicamente 16 o 32 filas) para calcular el gradiente, equilibrando velocidad y precisión.\n",
    "- Este enfoque permite un entrenamiento más eficiente al combinar las ventajas de los métodos anteriores.\n",
    "\n",
    "Terminología importante\n",
    "\n",
    "- **Época:** Se refiere a una pasada completa a través de todos los datos de entrenamiento. Es un hiperparámetro que se debe ajustar al implementar redes neuronales.\n",
    "\n",
    "- En SGD, se realizan tantas actualizaciones como filas hay en el conjunto de datos por cada época, mientras que en mini-batch, se realizan pasos basados en el tamaño del lote.\n",
    "\n",
    "1. **Lanzar desde todos los puntos a la vez (Descenso de Gradiente por Lotes Completos):** Aquí, miras todos tus lanzamientos y decides cómo mejorar. Es muy preciso porque consideras todo, pero puede ser lento, especialmente si hay muchos lanzamientos.\n",
    "\n",
    "2. **Lanzar una sola vez (Descenso de Gradiente Estocástico):** En este caso, lanzas una sola vez y ajustas tu técnica basándote solo en ese lanzamiento. Es rápido, pero a veces no te da la mejor idea de cómo mejorar, ya que solo ves un lanzamiento.\n",
    "\n",
    "3. **Lanzar en grupos pequeños (Descenso de Gradiente por Mini-Lotes):** Aquí, lanzas un pequeño grupo de veces, digamos 5 lanzamientos, y ajustas tu técnica. Es un buen equilibrio: no es tan lento como mirar todos los lanzamientos, pero tampoco es tan impreciso como lanzar solo una vez.\n",
    "\n",
    "**Épocas:** Ahora, cada vez que practicas lanzando desde todos los puntos, eso se llama una \"época\". Si lanzas 10 veces en una época, y decides ajustar tu técnica después de cada grupo de 5 lanzamientos, eso es como usar mini-batch.\n",
    "\n",
    "### Data Shuffling\n",
    "\n",
    "\"data shuffling\" y su importancia en el entrenamiento de modelos de redes neuronales, especialmente en el contexto de métodos de descenso de gradiente.\n",
    "\n",
    "**Conceptos de descenso de gradiente**\n",
    "\n",
    "Se utilizan métodos como el descenso de gradiente estocástico y el descenso de gradiente por mini-lotes para trabajar con subconjuntos de datos.\n",
    "Es recomendable mezclar los datos después de cada época para evitar movimientos cíclicos y facilitar la convergencia.\n",
    "\n",
    "**Importancia de las épocas y el \"shuffling\"**\n",
    "\n",
    "Una época se refiere a una pasada completa a través del conjunto de datos, y se realizan múltiples épocas durante el entrenamiento.\n",
    "Al mezclar los datos, se asegura que cada época no repita el mismo orden, lo que ayuda a explorar diferentes caminos hacia el valor óptimo.\n",
    "\n",
    "**Ejemplo de implementación**\n",
    "\n",
    "En el descenso de gradiente por mini-lotes, los datos se dividen en lotes específicos, y se calcula la derivada para ajustar los pesos en cada lote.\n",
    "Al finalizar una época, se comienza con un lote aleatorio diferente en la siguiente época, lo que contribuye a un mejor aprendizaje.\n",
    "\n",
    "---\n",
    "### Transform \n",
    "\n",
    "Este material se centra en la importancia de escalar las entradas en redes neuronales para asegurar que los pesos se actualicen de manera equilibrada y eficiente durante el proceso de entrenamiento.\n",
    "\n",
    "**Escalado de entradas**\n",
    "\n",
    "- Al actualizar los pesos en una red neuronal, es crucial normalizar las entradas para evitar que los valores más altos influyan desproporcionadamente en las actualizaciones de los pesos.\n",
    "- Si las entradas no están normalizadas, los valores más altos se actualizarán más rápidamente que los más bajos, lo que puede ralentizar la convergencia del modelo.\n",
    "\n",
    "**Métodos de escalado**\n",
    "\n",
    "- Se pueden utilizar diferentes métodos de escalado, como el escalado lineal al intervalo entre 0 y 1 (min-max scaling) o entre -1 y 1.\n",
    "- El escalador estándar también es útil, ya que permite que los valores se mantengan en la misma escala, lo que es especialmente importante al usar funciones como la sigmoide o la tangente hiperbólica.\n",
    "\n",
    "**Reforzando el aprendizaje**\n",
    "\n",
    "- Es fundamental preparar y preprocesar los datos adecuadamente para los modelos de redes neuronales, incluyendo la codificación one-hot y la función softmax para clasificación multiclase.\n",
    "- Recuerda que la normalización de las entradas es clave para un entrenamiento efectivo y equilibrado de tu modelo. ¡Sigue adelante, estás haciendo un gran trabajo! Si tienes más preguntas, aquí estoy para ayudarte."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
